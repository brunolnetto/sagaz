# Lakehouse Ingestion Saga

Demonstrates the medallion architecture (Bronze â†’ Silver â†’ Gold) with layer-by-layer rollback.

## Use Case

Data lakehouses use a multi-layer architecture:
- **Bronze**: Raw data, exactly as received
- **Silver**: Cleaned, deduplicated, validated
- **Gold**: Aggregated, business-ready analytics

**The Problem**: Failures mid-pipeline can leave orphaned data in intermediate layers.

**The Solution**: Sagaz rolls back each layer in reverse order on failure.

## Medallion Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Data Lakehouse                              â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚   BRONZE   â”‚â”€â”€â”€â”€â–¶â”‚   SILVER   â”‚â”€â”€â”€â”€â–¶â”‚    GOLD    â”‚          â”‚
â”‚  â”‚   (Raw)    â”‚     â”‚  (Clean)   â”‚     â”‚   (Agg)    â”‚          â”‚
â”‚  â”‚            â”‚     â”‚            â”‚     â”‚            â”‚          â”‚
â”‚  â”‚ â€¢ As-is    â”‚     â”‚ â€¢ Deduped  â”‚     â”‚ â€¢ Metrics  â”‚          â”‚
â”‚  â”‚ â€¢ +Metadataâ”‚     â”‚ â€¢ Validatedâ”‚     â”‚ â€¢ KPIs     â”‚          â”‚
â”‚  â”‚ â€¢ Append   â”‚     â”‚ â€¢ Typed    â”‚     â”‚ â€¢ Dims     â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                                 â”‚
â”‚  Compensation:       Compensation:       Compensation:          â”‚
â”‚  Delete partition    Delete partition    Delete partition       â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Pipeline Steps

```
Source â†’ [Bronze: Ingest] â†’ [Silver: Clean] â†’ [Gold: Aggregate]
                                                       â”‚
         [Notify] â† [Update Catalog] â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Running the Example

```bash
python -m examples.data_engineering.lakehouse_ingestion.main
```

## Example Output

```
ğŸ  Lakehouse Ingestion Saga - Medallion Architecture Demo
================================================================================
    Bronze (Raw) â†’ Silver (Clean) â†’ Gold (Aggregated)
================================================================================
ğŸ¥‰ BRONZE: Ingesting raw data
   Source: s3://data-lake-raw/events/clickstream/
   Target: bronze.raw_clickstream
âœ… BRONZE complete: 145,231 records from 25 files

ğŸ¥ˆ SILVER: Processing and cleaning data
   Source: bronze.raw_clickstream
   Target: silver.cleaned_clickstream
   Duplicates removed: 5,234
   Nulls filled: 2,145
   Invalid records removed: 876
âœ… SILVER complete: 139,121 records (95.8% retained)

ğŸ¥‡ GOLD: Aggregating for analytics
   Source: silver.cleaned_clickstream
   Target: gold.clickstream_metrics
   Aggregations: ['daily_event_counts', 'hourly_user_activity', ...]
âœ… GOLD complete: 1,234 aggregated records

ğŸ“š Updating data catalog
   Registered: bronze.raw_clickstream/partition_date=2026-01-06
   Registered: silver.cleaned_clickstream/partition_date=2026-01-06
   Registered: gold.clickstream_metrics/partition_date=2026-01-06
âœ… Catalog updated with lineage

ğŸ“¢ Notifying downstream consumers
âœ… Notified 3 downstream consumers
```

## Compensation Flow

When Gold aggregation fails:

```
aggregate_to_gold FAILS
    â†“
delete_gold_partition (compensation) - nothing to delete
    â†“
delete_silver_partition (compensation) - 139,121 records removed
    â†“
delete_bronze_partition (compensation) - 145,231 records removed
    â†“
All layers rolled back âœ…
```

## Integration with Delta Lake / Iceberg

```python
from delta import DeltaTable
from examples.data_engineering.lakehouse_ingestion import LakehouseIngestionSaga

class DeltaLakehouseSaga(LakehouseIngestionSaga):
    
    @action("process_to_silver", depends_on=["ingest_to_bronze"])
    async def process_to_silver(self, ctx):
        # Use Delta Lake for ACID transactions
        bronze_df = spark.read.format("delta").load(ctx["bronze_path"])
        
        silver_df = (bronze_df
            .dropDuplicates(["event_id"])
            .filter("user_id IS NOT NULL")
            .withColumn("processed_at", current_timestamp()))
        
        silver_df.write.format("delta").mode("overwrite").save(ctx["silver_path"])
        
        return {"silver_record_count": silver_df.count(), ...}
    
    @compensate("process_to_silver")
    async def delete_silver_partition(self, ctx):
        # Delta Lake time travel for rollback
        DeltaTable.forPath(spark, ctx["silver_path"]).restoreToVersion(0)
```

## Key Benefits

| Benefit | Description |
|---------|-------------|
| **Layer consistency** | All layers roll back together |
| **No orphaned data** | Failed ingestions leave no traces |
| **Data lineage** | Track data flow across layers |
| **Quality metrics** | Know exactly what was cleaned |
| **Downstream sync** | Consumers notified only on success |
